{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import csv\nimport re\nimport math\nfrom collections import Counter\nimport random \n\nwith open('rt_reviews.csv', 'r', newline='', encoding='cp1252') as csvfile:\n    reader = csv.DictReader(csvfile)\n    merged_data = []\n    for row in reader:\n        review = row['Review'].lower()\n        row['Review'] = review\n        merged_data.append(row)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Divide the dataset into train, development and test sets\nrandom.shuffle(merged_data)\ntrain_data = merged_data[:int(0.8*len(merged_data))]\ndev_data = merged_data[int(0.8*len(merged_data)):int(0.9*len(merged_data))]\ntest_data = merged_data[int(0.9*len(merged_data)):]\n\n\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Build a vocabulary as a list\nword_freq = {}\nfor review in train_data:\n    words = review['Review'].split()\n    for word in words:\n        if word in word_freq:\n            word_freq[word] += 1\n        else:\n            word_freq[word] = 1\n\nvocabulary = [word for word in word_freq if word_freq[word] >= 5]\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create a reverse index\nreverse_index = {}\nfor i, word in enumerate(vocabulary):\n    reverse_index[word] = i",
      "metadata": {
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Part C\n# Count the number of documents containing 'the'\ndoc_containing_the = 0\nfor review in train_data:\n    if 'the' in review['Review']:\n        doc_containing_the += 1\n\n# Calculate the probability of occurrence\nP_the = doc_containing_the / len(train_data)\n\nprint(f\"Probability of occurrence of 'the': {P_the:.4f}\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "text": "Probability of occurrence of 'the': 0.7131\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "# Count the number of positive reviews containing 'the'\npos_doc_containing_the = 0\nfor review in train_data:\n    if review['Freshness'] == 'fresh' and 'the' in review['Review']:\n        pos_doc_containing_the += 1\n\n# Calculate the number of positive reviews\nnum_pos_docs = sum([1 for review in train_data if review['Freshness'] == 'fresh'])\n\n# Calculate the conditional probability\nif num_pos_docs != 0:\n    P_the_given_pos = pos_doc_containing_the / num_pos_docs\nelse:\n    P_the_given_pos = 0\n\nprint(f\"Conditional probability of 'the' given Positive: {P_the_given_pos:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": "Conditional probability of 'the' given Positive: 0.7068\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import math\n\nclass NaiveBayesClassifier:\n    def __init__(self):\n        self.vocab = set()\n        self.class_prior = {}\n        self.word_freq = {}\n        self.class_word_count = {}\n        self.class_total_count = {}\n        self.num_classes = 0\n\n    def tokenize(self, review):\n        # convert to lowercase and split into words\n        words = review.lower().split()\n        # remove any non-alphanumeric characters\n        words = [word.strip('.,!?()[]{}') for word in words]\n        # remove any empty strings\n        words = [word for word in words if len(word) > 0]\n        return words\n    def fit(self, reviews, labels):\n        self.num_classes = len(set(labels))\n        # compute class prior probabilities\n        for label in labels:\n            if label not in self.class_prior:\n                self.class_prior[label] = 0\n            self.class_prior[label] += 1\n        for label in self.class_prior:\n            self.class_prior[label] /= len(labels)\n        # compute word frequency and class word count\n        for i in range(len(reviews)):\n            words = self.tokenize(reviews[i])\n            label = labels[i]\n            self.class_word_count[label] = self.class_word_count.get(label, 0) + len(words)\n            self.class_total_count[label] = self.class_total_count.get(label, 0) + 1\n            for word in words:\n                self.vocab.add(word)\n                if word not in self.word_freq:\n                    self.word_freq[word] = {}\n                if label not in self.word_freq[word]:\n                    self.word_freq[word][label] = 0\n                self.word_freq[word][label] += 1\n\n    def predict(self, reviews):\n        predictions = []\n        for review in reviews:\n            words = self.tokenize(review)\n            class_scores = {}\n            for label in self.class_prior:\n                score = math.log(self.class_prior[label])\n                for word in words:\n                    if word in self.vocab and label in self.word_freq[word]:\n                        word_prob = self.word_freq[word][label] / self.class_word_count[label]\n                        score += math.log(word_prob)\n                class_scores[label] = score\n            pred_label = max(class_scores, key=class_scores.get)\n            predictions.append(pred_label)\n        return predictions",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model = NaiveBayesClassifier()\ntrain_reviews = [row['Review'] for row in train_data]\ntrain_labels = [row['Freshness'] for row in train_data]\nmodel.fit(train_reviews, train_labels)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# make predictions on the dev set\ndev_reviews = [row['Review'] for row in dev_data]\ndev_labels = [row['Freshness'] for row in dev_data]\ndev_predictions = model.predict(dev_reviews)\naccuracy = sum(1 for i in range(len(dev_labels)) if dev_labels[i] == dev_predictions[i]) / len(dev_labels)\nprint(f\"Accuracy on dev data: {accuracy}\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}